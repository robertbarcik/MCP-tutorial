{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP with Local Open Source Models\n",
    "\n",
    "This notebook demonstrates how to use MCP servers with a **local open source language model** instead of commercial APIs.\n",
    "\n",
    "## Why Run Models Locally?\n",
    "\n",
    "‚úÖ **Privacy** - Company data never leaves your machine  \n",
    "‚úÖ **Cost** - No API fees (one-time compute cost)  \n",
    "‚úÖ **Offline** - Works without internet after download  \n",
    "‚úÖ **Control** - Full transparency, can customize  \n",
    "‚úÖ **Learning** - Understand tool calling mechanics  \n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Load and run **Gemma 2 2B** locally (lightweight and fast!)\n",
    "2. Connect the model to MCP server functions\n",
    "3. Implement tool calling from scratch\n",
    "4. Process natural language queries end-to-end\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- GPU recommended (Google Colab free tier works perfectly!)\n",
    "- ~2-3GB VRAM for model (much lighter than larger models)\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Model Loading\n",
    "\n",
    "### 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q transformers torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Note: Running on CPU will be significantly slower\")\n",
    "    print(\"   Consider using Google Colab with GPU runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Gemma 2 2B\n",
    "\n",
    "**Why this model?**  \n",
    "Gemma 2 2B is Google's lightweight, instruction-tuned model that's perfect for Google Colab. It's specifically optimized for:\n",
    "- **Speed** - Much faster loading and inference than 7B models\n",
    "- **Memory efficiency** - Runs smoothly on Colab's free tier GPU\n",
    "- **Tool calling** - Supports function calling and structured outputs\n",
    "- **Quality** - Despite its small size, performs well on instruction-following tasks\n",
    "\n",
    "**Note:** First run will download ~5GB (cached for future use). With 4-bit quantization, it uses only ~2-3GB VRAM - perfect for Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"üì• Loading Gemma 2 2B model...\")\n",
    "print(\"   This will download ~5GB on first run (cached for future use)\")\n",
    "print(\"   Please wait 1-2 minutes...\\n\")\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"   ‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "print(\"\\nü§ñ Loading model with 4-bit quantization...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Quantization type (nf4 or fp4)\n",
    "    bnb_4bit_use_double_quant=True        # Nested quantization for better memory efficiency\n",
    ")\n",
    "\n",
    "# Load model with quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",              # Automatically place on GPU\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "print(\"   ‚úÖ Model loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Gemma 2 2B is ready to use!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Import MCP Server Functions\n",
    "\n",
    "We'll import functions directly from all five MCP servers (same approach as Notebook 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket Server\n",
    "from ticket_server import (\n",
    "    search_tickets,\n",
    "    get_ticket_details,\n",
    "    get_ticket_metrics,\n",
    "    find_similar_tickets_to\n",
    ")\n",
    "\n",
    "# Customer Server\n",
    "from customer_server import (\n",
    "    lookup_customer,\n",
    "    check_customer_status,\n",
    "    get_sla_terms,\n",
    "    list_customer_contacts\n",
    ")\n",
    "\n",
    "# Billing Server\n",
    "from billing_server import (\n",
    "    get_invoice,\n",
    "    check_payment_status,\n",
    "    get_billing_history,\n",
    "    calculate_outstanding_balance\n",
    ")\n",
    "\n",
    "# Knowledge Base Server\n",
    "from kb_server import (\n",
    "    search_solutions,\n",
    "    get_article,\n",
    "    get_related_articles,\n",
    "    get_common_fixes\n",
    ")\n",
    "\n",
    "# Asset Server\n",
    "from asset_server import (\n",
    "    lookup_asset,\n",
    "    check_warranty,\n",
    "    get_asset_history,\n",
    "    list_assets_by_customer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All MCP server functions imported successfully!\")\n",
    "print(\"   Total: 20 tools available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Tool Registry\n",
    "\n",
    "We need to describe each tool in a format the model understands. Gemma 2 2B works well with OpenAI-style function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define available tools in OpenAI function calling format\n",
    "# Most instruction-tuned models understand this standard format\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_tickets\",\n",
    "            \"description\": \"Search for support tickets with optional filters for status, priority, customer_id, or keywords\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"status\": {\"type\": \"string\", \"description\": \"Filter by status: open, in_progress, resolved, closed\"},\n",
    "                    \"priority\": {\"type\": \"string\", \"description\": \"Filter by priority: low, medium, high, critical\"},\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Filter by customer ID (e.g., CUST-001)\"},\n",
    "                    \"keyword\": {\"type\": \"string\", \"description\": \"Search in subject and description\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticket_details\",\n",
    "            \"description\": \"Get detailed information about a specific ticket by its ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticket_id\": {\"type\": \"string\", \"description\": \"Ticket ID (e.g., TKT-1001)\"}\n",
    "                },\n",
    "                \"required\": [\"ticket_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticket_metrics\",\n",
    "            \"description\": \"Get ticket statistics and metrics for a time period\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"period\": {\"type\": \"string\", \"description\": \"Time period: last_7_days, last_30_days, last_90_days\"}\n",
    "                },\n",
    "                \"required\": [\"period\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_customer\",\n",
    "            \"description\": \"Look up customer information by customer ID or company name\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID (e.g., CUST-001)\"},\n",
    "                    \"company_name\": {\"type\": \"string\", \"description\": \"Company name to search for\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_sla_terms\",\n",
    "            \"description\": \"Get SLA (Service Level Agreement) terms for a customer\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID (e.g., CUST-001)\"}\n",
    "                },\n",
    "                \"required\": [\"customer_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_invoice\",\n",
    "            \"description\": \"Get invoice information by invoice ID or customer ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"invoice_id\": {\"type\": \"string\", \"description\": \"Invoice ID (e.g., INV-1001)\"},\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID to get all invoices\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_outstanding_balance\",\n",
    "            \"description\": \"Calculate total outstanding and overdue balance for a customer\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID (e.g., CUST-001)\"}\n",
    "                },\n",
    "                \"required\": [\"customer_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_solutions\",\n",
    "            \"description\": \"Search knowledge base articles by keyword or topic\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "                    \"limit\": {\"type\": \"integer\", \"description\": \"Maximum number of results (default 5)\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_article\",\n",
    "            \"description\": \"Get full knowledge base article by article ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"article_id\": {\"type\": \"string\", \"description\": \"Article ID (e.g., KB-001)\"}\n",
    "                },\n",
    "                \"required\": [\"article_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_asset\",\n",
    "            \"description\": \"Look up asset information by asset ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"asset_id\": {\"type\": \"string\", \"description\": \"Asset ID (e.g., AST-SRV-001)\"}\n",
    "                },\n",
    "                \"required\": [\"asset_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"check_warranty\",\n",
    "            \"description\": \"Check warranty status and details for an asset\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"asset_id\": {\"type\": \"string\", \"description\": \"Asset ID (e.g., AST-SRV-001)\"}\n",
    "                },\n",
    "                \"required\": [\"asset_id\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a mapping from tool names to actual Python functions\n",
    "tool_map = {\n",
    "    \"search_tickets\": search_tickets,\n",
    "    \"get_ticket_details\": get_ticket_details,\n",
    "    \"get_ticket_metrics\": get_ticket_metrics,\n",
    "    \"lookup_customer\": lookup_customer,\n",
    "    \"get_sla_terms\": get_sla_terms,\n",
    "    \"get_invoice\": get_invoice,\n",
    "    \"calculate_outstanding_balance\": calculate_outstanding_balance,\n",
    "    \"search_solutions\": search_solutions,\n",
    "    \"get_article\": get_article,\n",
    "    \"lookup_asset\": lookup_asset,\n",
    "    \"check_warranty\": check_warranty\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Tool registry created with {len(tools)} tools\")\n",
    "print(\"\\nAvailable tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool['function']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Tool Calling Implementation\n",
    "\n",
    "### 3.1 How It Works (Brief Review)\n",
    "\n",
    "You already know the tool calling loop from previous notebooks:\n",
    "\n",
    "1. **User asks a question** ‚Üí Model receives query + available tools\n",
    "2. **Model decides what to do** ‚Üí Either call tools OR give final answer\n",
    "3. **Execute tools** ‚Üí Run Python functions, get results\n",
    "4. **Feed results back** ‚Üí Model uses results to answer\n",
    "5. **Repeat if needed** ‚Üí Model might call more tools\n",
    "\n",
    "The key difference here: we're implementing this manually with a local model instead of using an API service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Helper Functions\n",
    "\n",
    "First, let's create helper functions for executing tools and generating model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool(tool_name, arguments):\n",
    "    \"\"\"\n",
    "    Execute a tool by name with given arguments.\n",
    "    \n",
    "    Args:\n",
    "        tool_name: Name of the tool to execute\n",
    "        arguments: Dictionary of arguments to pass\n",
    "        \n",
    "    Returns:\n",
    "        Tool execution result (dict or error message)\n",
    "    \"\"\"\n",
    "    if tool_name not in tool_map:\n",
    "        return {\"error\": f\"Tool '{tool_name}' not found\"}\n",
    "    \n",
    "    try:\n",
    "        # Call the actual Python function\n",
    "        result = tool_map[tool_name](**arguments)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Tool execution failed: {str(e)}\"}\n",
    "\n",
    "\n",
    "def generate_response(messages, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Generate a response from the model given conversation history.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    # Format messages using the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens (not the input)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def parse_tool_calls(response):\n",
    "    \"\"\"\n",
    "    Parse tool calls from model response.\n",
    "    Different models format tool calls differently - this function tries to handle multiple formats.\n",
    "    \n",
    "    Args:\n",
    "        response: Model's text response\n",
    "        \n",
    "    Returns:\n",
    "        List of tool call dictionaries, or None if no tool calls\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try format 1: <tool_call> tags (used by some models like Hermes)\n",
    "        if \"<tool_call>\" in response:\n",
    "            start = response.find(\"<tool_call>\") + len(\"<tool_call>\")\n",
    "            end = response.find(\"</tool_call>\")\n",
    "            json_str = response[start:end].strip()\n",
    "            \n",
    "            # Parse the JSON\n",
    "            tool_calls = json.loads(json_str)\n",
    "            \n",
    "            # Ensure it's a list\n",
    "            if isinstance(tool_calls, dict):\n",
    "                tool_calls = [tool_calls]\n",
    "            \n",
    "            return tool_calls\n",
    "        \n",
    "        # Try format 2: Look for JSON blocks with \"name\" and \"arguments\" keys\n",
    "        # This is a fallback for models that output JSON directly\n",
    "        import re\n",
    "        # Find all JSON-like structures in the response\n",
    "        json_pattern = r'\\{[^{}]*\"name\"\\s*:\\s*\"[^\"]+\"\\s*,\\s*\"arguments\"\\s*:\\s*\\{[^}]*\\}[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, response, re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            tool_calls = []\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    tool_call = json.loads(match)\n",
    "                    if \"name\" in tool_call:\n",
    "                        tool_calls.append(tool_call)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if tool_calls:\n",
    "                return tool_calls\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error parsing tool calls: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Main Query Function\n",
    "\n",
    "This implements the complete tool calling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_tools(user_question, max_iterations=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Process a user query with tool calling capability.\n",
    "    \n",
    "    Args:\n",
    "        user_question: Natural language question from user\n",
    "        max_iterations: Maximum number of tool calling rounds\n",
    "        verbose: Print intermediate steps\n",
    "        \n",
    "    Returns:\n",
    "        Final answer from the model\n",
    "    \"\"\"\n",
    "    # Build system message with tool definitions\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"You are a helpful IT support assistant with access to multiple tools.\n",
    "\n",
    "When you need information to answer a question, use the available tools by generating a tool call in this format:\n",
    "<tool_call>\n",
    "{{\n",
    "  \"name\": \"tool_name\",\n",
    "  \"arguments\": {{\n",
    "    \"param1\": \"value1\",\n",
    "    \"param2\": \"value2\"\n",
    "  }}\n",
    "}}\n",
    "</tool_call>\n",
    "\n",
    "You can call multiple tools by providing a list of tool calls.\n",
    "\n",
    "Available tools:\n",
    "{json.dumps([t['function'] for t in tools], indent=2)}\n",
    "\n",
    "After receiving tool results, use them to provide a helpful answer to the user.\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Initialize conversation\n",
    "    messages = [\n",
    "        system_message,\n",
    "        {\"role\": \"user\", \"content\": user_question}\n",
    "    ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"üë§ User: {user_question}\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # Tool calling loop\n",
    "    for iteration in range(max_iterations):\n",
    "        if verbose:\n",
    "            print(f\"\\nüîÑ Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Get model response\n",
    "        response = generate_response(messages)\n",
    "        \n",
    "        # Check if model wants to call tools\n",
    "        tool_calls = parse_tool_calls(response)\n",
    "        \n",
    "        if tool_calls:\n",
    "            # Model wants to use tools\n",
    "            if verbose:\n",
    "                print(f\"üîß Model requested {len(tool_calls)} tool call(s)\")\n",
    "            \n",
    "            # Add assistant's response to history\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            # Execute each tool\n",
    "            tool_results = []\n",
    "            for tool_call in tool_calls:\n",
    "                tool_name = tool_call.get(\"name\")\n",
    "                arguments = tool_call.get(\"arguments\", {})\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\n  Calling: {tool_name}\")\n",
    "                    print(f\"  Args: {json.dumps(arguments, indent=4)}\")\n",
    "                \n",
    "                # Execute the tool\n",
    "                result = execute_tool(tool_name, arguments)\n",
    "                tool_results.append(result)\n",
    "                \n",
    "                if verbose:\n",
    "                    result_str = json.dumps(result, indent=4)\n",
    "                    preview = result_str[:200] + \"...\" if len(result_str) > 200 else result_str\n",
    "                    print(f\"  Result: {preview}\")\n",
    "            \n",
    "            # Add tool results to conversation\n",
    "            tool_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Tool results:\\n{json.dumps(tool_results, indent=2)}\"\n",
    "            }\n",
    "            messages.append(tool_message)\n",
    "            \n",
    "            # Continue loop to get next response\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            # No tool calls - this is the final answer\n",
    "            if verbose:\n",
    "                print(\"\\n‚úÖ Final answer received\")\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(f\"ü§ñ Assistant: {response}\")\n",
    "                print(\"=\"*70)\n",
    "            \n",
    "            return response\n",
    "    \n",
    "    # Hit max iterations\n",
    "    return \"I've reached the maximum number of tool calls. Please try simplifying your question.\"\n",
    "\n",
    "print(\"‚úÖ Main query function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Examples & Demonstrations\n",
    "\n",
    "Now let's test the system with various queries!\n",
    "\n",
    "### Example 1: Simple Single-Tool Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"What are all the critical priority tickets?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "1. Model analyzed the question\n",
    "2. Decided to call `search_tickets` with `priority=\"critical\"`\n",
    "3. Received the results\n",
    "4. Formatted a natural language response\n",
    "\n",
    "### Example 2: Multi-Tool Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Show me customer CUST-001's information and their SLA terms\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "- Model called TWO tools: `lookup_customer` AND `get_sla_terms`\n",
    "- Combined the information into a coherent answer\n",
    "\n",
    "### Example 3: Complex Multi-Step Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Which customers have critical tickets? Show their outstanding balances.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "1. First tool call: `search_tickets(priority=\"critical\")` to find critical tickets\n",
    "2. Extracted customer IDs from ticket results\n",
    "3. Multiple tool calls: `calculate_outstanding_balance` for each customer\n",
    "4. Correlated the data and formatted the answer\n",
    "\n",
    "This demonstrates **multi-step reasoning** - the model chains tools together!\n",
    "\n",
    "### Example 4: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"Get details for ticket TKT-9999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "- Tool returned an error (ticket not found)\n",
    "- Error included helpful hints (from your MCP server design!)\n",
    "- Model used the hints to provide a helpful response to the user\n",
    "\n",
    "### Example 5: Knowledge Base Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Find articles about Windows blue screen errors and show me the most relevant one\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Asset and Warranty Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Check the warranty status for asset AST-SRV-001. Is it still valid?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Try It Yourself!\n",
    "\n",
    "Now it's your turn. Use the `query_with_tools()` function to ask your own questions.\n",
    "\n",
    "### Your Custom Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own query here!\n",
    "my_query = \"Your question here...\"\n",
    "\n",
    "answer = query_with_tools(my_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Queries to Try\n",
    "\n",
    "```python\n",
    "# Cross-server correlation\n",
    "query_with_tools(\"Show me all high priority tickets for customers with overdue invoices\")\n",
    "\n",
    "# Metrics and analytics\n",
    "query_with_tools(\"What are the ticket metrics for the last 30 days?\")\n",
    "\n",
    "# Customer insights\n",
    "query_with_tools(\"Tell me about customer CUST-002, their tickets, and billing status\")\n",
    "\n",
    "# Knowledge base assistance\n",
    "query_with_tools(\"What are common fixes for network connectivity issues?\")\n",
    "\n",
    "# Asset management\n",
    "query_with_tools(\"Which assets have expired warranties?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Understanding Local vs API Models\n",
    "\n",
    "### Key Differences You Experienced\n",
    "\n",
    "| Aspect | Local Model (Gemma 2 2B) | API Model (OpenAI) |\n",
    "|--------|---------------------------|--------------------|\n",
    "| **Privacy** | ‚úÖ All data stays local | ‚ö†Ô∏è Sent to external servers |\n",
    "| **Cost** | ‚úÖ Free (after setup) | üí∞ Pay per token |\n",
    "| **Speed** | ‚úÖ Fast (especially 2B models) | ‚úÖ Fast (optimized infrastructure) |\n",
    "| **Quality** | ‚ö†Ô∏è Good for simple tasks, may struggle with complex reasoning | ‚úÖ Excellent reasoning |\n",
    "| **Offline** | ‚úÖ Works without internet | ‚ùå Requires connection |\n",
    "| **Setup** | ‚ö†Ô∏è More complex | ‚úÖ Simple API key |\n",
    "| **Customization** | ‚úÖ Can fine-tune, modify | ‚ùå Limited control |\n",
    "| **Memory** | ‚úÖ 2-3GB VRAM (Colab friendly!) | N/A (runs on their servers) |\n",
    "\n",
    "### When to Use Local Models\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Sensitive data (healthcare, finance, legal)\n",
    "- High-volume queries (cost savings)\n",
    "- Offline/air-gapped environments\n",
    "- Learning and experimentation\n",
    "- Custom fine-tuning needs\n",
    "- Limited GPU resources (2B models are very efficient!)\n",
    "\n",
    "‚ö†Ô∏è **Consider APIs when:**\n",
    "- Need highest quality reasoning\n",
    "- Complex multi-step tasks\n",
    "- Want minimal setup complexity\n",
    "- Need consistent high-quality responses\n",
    "\n",
    "### Potential Limitations You Might Notice\n",
    "\n",
    "1. **Tool Selection Errors**: Smaller models (2B) may occasionally:\n",
    "   - Choose wrong tool\n",
    "   - Hallucinate tool names\n",
    "   - Miss required parameters\n",
    "   - Need more explicit instructions\n",
    "\n",
    "2. **Response Quality**: May be less natural or detailed than larger models\n",
    "\n",
    "3. **Complex Reasoning**: Multi-step queries might require more guidance or simpler prompts\n",
    "\n",
    "4. **Consistency**: May vary more in quality compared to larger models\n",
    "\n",
    "**Note:** These trade-offs are why hybrid approaches exist - use local for routine queries, API for complex cases. Gemma 2 2B is particularly good for learning and simple tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced Exercises\n",
    "\n",
    "### Exercise 1: Add Conversation History\n",
    "\n",
    "Modify `query_with_tools()` to support multi-turn conversations:\n",
    "```python\n",
    "# First query\n",
    "history = []\n",
    "answer1, history = query_with_tools_stateful(\"Show me critical tickets\", history)\n",
    "\n",
    "# Follow-up query (model remembers context)\n",
    "answer2, history = query_with_tools_stateful(\"What are their customer IDs?\", history)\n",
    "```\n",
    "\n",
    "### Exercise 2: Implement Confidence Scoring\n",
    "\n",
    "Add logic to detect when the model is uncertain and ask for clarification:\n",
    "- Parse for uncertainty phrases (\"I'm not sure\", \"might be\", etc.)\n",
    "- Request confirmation before executing potentially wrong tools\n",
    "\n",
    "### Exercise 3: Tool Call Validation\n",
    "\n",
    "Add a validation layer:\n",
    "- Check if tool exists before calling\n",
    "- Validate parameter types match expected schema\n",
    "- Provide better error messages to model when validation fails\n",
    "\n",
    "### Exercise 4: Compare Different Models\n",
    "\n",
    "Try swapping in a different model to compare:\n",
    "- **Gemma 2 9B** (`google/gemma-2-9b-it`) - Same family, better quality\n",
    "- **Qwen2.5-7B-Instruct** - Good alternative in the 7B range\n",
    "- **Phi-3-mini** (`microsoft/Phi-3-mini-4k-instruct`) - Another efficient small model\n",
    "- **Llama-3.2-3B** - Meta's small instruction model\n",
    "\n",
    "Compare:\n",
    "- Tool selection accuracy\n",
    "- Response quality\n",
    "- Speed\n",
    "- Memory usage\n",
    "\n",
    "**Tip**: Just change the `model_name` variable in cell 6!\n",
    "\n",
    "### Exercise 5: Performance Optimization\n",
    "\n",
    "Optimize for speed:\n",
    "- Reduce `max_new_tokens` for tool calls (tool calls don't need long responses)\n",
    "- Implement caching for repeated queries\n",
    "- Batch multiple tool calls when possible\n",
    "- Try removing quantization if you have enough VRAM (faster inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "‚úÖ **Local Model Setup**\n",
    "   - Loading Gemma 2 2B with 4-bit quantization\n",
    "   - GPU optimization for inference\n",
    "   - Memory management in Colab (only 2-3GB VRAM needed!)\n",
    "   - Fast loading and inference times\n",
    "\n",
    "‚úÖ **Tool Calling Implementation**\n",
    "   - Creating tool schemas in OpenAI function format\n",
    "   - Parsing model outputs for tool calls\n",
    "   - Executing Python functions dynamically\n",
    "   - Multi-turn tool calling loops\n",
    "   - Handling different model output formats\n",
    "\n",
    "‚úÖ **Practical Applications**\n",
    "   - Privacy-preserving AI for sensitive data\n",
    "   - Cost-effective alternative to API services\n",
    "   - Understanding tool calling mechanics\n",
    "   - Running on free-tier Google Colab\n",
    "\n",
    "‚úÖ **Trade-offs and Decisions**\n",
    "   - When to use local vs API models\n",
    "   - Performance vs quality considerations\n",
    "   - Memory efficiency with smaller models\n",
    "   - Real-world deployment scenarios\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Privacy Matters**: Local models enable AI on sensitive data without external dependencies\n",
    "\n",
    "2. **Tool Calling is Universal**: Same concepts work across OpenAI, Anthropic, and open source models\n",
    "\n",
    "3. **Size vs Performance**: Smaller models (2B) can handle tool calling with the right setup, though with some quality trade-offs\n",
    "\n",
    "4. **Open Source = Control**: Full transparency and customization options\n",
    "\n",
    "5. **Implementation Matters**: Prompt engineering and error handling crucial for success\n",
    "\n",
    "6. **Efficiency First**: Gemma 2 2B proves you don't always need large models - 2B parameters can be surprisingly capable!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Fine-tune Gemma** on your specific tools/domain for better accuracy\n",
    "- **Deploy in production** with proper error handling and monitoring  \n",
    "- **Explore other models** (try Gemma 2 9B for better quality, or Qwen 2.5 for comparison)\n",
    "- **Build hybrid systems** (local for routine queries, API for complex cases)\n",
    "- **Optimize prompts** to get better tool selection from smaller models\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You now know how to build lightweight, privacy-preserving AI assistants with tool calling capabilities using small open source models that run perfectly on Google Colab!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
