{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUGvzG0euwTc"
   },
   "source": [
    "# MCP with Local Open Source Models\n",
    "\n",
    "This notebook demonstrates how to use MCP servers with a **local open source language model** instead of commercial APIs.\n",
    "\n",
    "## Why Run Models Locally?\n",
    "\n",
    "‚úÖ **Privacy** - Company data never leaves your machine  \n",
    "‚úÖ **Cost** - No API fees (one-time compute cost)  \n",
    "‚úÖ **Offline** - Works without internet after download  \n",
    "‚úÖ **Control** - Full transparency, can customize  \n",
    "‚úÖ **Learning** - Understand tool calling mechanics  \n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Load and run **Gemma 2 2B** locally (lightweight and fast!)\n",
    "2. Connect the model to MCP server functions\n",
    "3. Implement tool calling from scratch\n",
    "4. Process natural language queries end-to-end\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- GPU recommended (Google Colab free tier works perfectly!)\n",
    "- ~2-3GB VRAM for model (much lighter than larger models)\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0HiC6YruwTd"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Model Loading\n",
    "\n",
    "### 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHUUzvZPuwTe",
    "outputId": "9433af58-5cff-4374-808e-936c48c5ea84"
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q transformers torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTEsPcgpuwTf"
   },
   "source": [
    "### 1.2 Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wrXPznFuwTf",
    "outputId": "9b6fa245-911f-4d3b-c1b6-63b6efe77024"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Note: Running on CPU will be significantly slower\")\n",
    "    print(\"   Consider using Google Colab with GPU runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNymMqlruwTf"
   },
   "source": [
    "### 1.3 About Gemma 2 2B\n",
    "\n",
    "**Why this model?**  \n",
    "Gemma 2 2B is Google's lightweight, instruction-tuned model that's perfect for Google Colab. It's specifically optimized for:\n",
    "- **Speed** - Much faster loading and inference than 7B models\n",
    "- **Memory efficiency** - Runs smoothly on Colab's free tier GPU\n",
    "- **Tool calling** - Supports function calling and structured outputs\n",
    "- **Quality** - Despite its small size, performs well on instruction-following tasks\n",
    "\n",
    "**Note:** First run will download ~5GB (cached for future use). With 4-bit quantization, it uses only ~2-3GB VRAM - perfect for Colab!\n",
    "\n",
    "‚ö†Ô∏è **Important**: Gemma models require authentication - follow the steps in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz8DNK3ruwTf"
   },
   "source": [
    "### 1.5 Load Gemma 2 2B Model\n",
    "\n",
    "Now that you're authenticated, let's load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfbkmeOLuwTf",
    "outputId": "5cc4298a-1d56-4b5d-f525-554ea9b5276b"
   },
   "outputs": [],
   "source": [
    "# Load Hugging Face token from Colab secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úÖ Hugging Face token loaded successfully from Colab secrets!\")\n",
    "    print(f\"   Token preview: {HF_TOKEN[:10]}...\" if HF_TOKEN else \"   ‚ö†Ô∏è Token is empty!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error loading token from Colab secrets!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"\\nüí° Make sure you:\")\n",
    "    print(\"   1. Created a Hugging Face token at https://huggingface.co/settings/tokens\")\n",
    "    print(\"   2. Added it to Colab secrets with name 'HF_TOKEN'\")\n",
    "    print(\"   3. Enabled 'Notebook access' for the secret\")\n",
    "    HF_TOKEN = None\n",
    "\n",
    "# Verify token is available\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"‚ùå No HF_TOKEN found! Please follow the authentication steps above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg_6eTqKuwTg"
   },
   "source": [
    "### 1.4 Authentication Setup (Required for Gemma Models)\n",
    "\n",
    "‚ö†Ô∏è **Important**: Gemma models are gated and require authentication. Follow these steps **before** running the next cell:\n",
    "\n",
    "#### Step 1: Accept the Gemma License\n",
    "\n",
    "1. Go to https://huggingface.co/google/gemma-2-2b-it\n",
    "2. If you don't have a Hugging Face account, click **Sign Up** (it's free)\n",
    "3. Log in to your account\n",
    "4. Click the **\"Agree and access repository\"** button to accept the license terms\n",
    "5. Wait for approval (usually instant, but may take 1-2 minutes)\n",
    "\n",
    "#### Step 2: Set Up Authentication in Google Colab\n",
    "\n",
    "Now you need to authenticate using a Hugging Face token:\n",
    "\n",
    "**2.1 Create a Hugging Face Token:**\n",
    "1. Go to https://huggingface.co/settings/tokens\n",
    "2. Click **\"New token\"**\n",
    "3. Give it a name (e.g., \"colab-access\")\n",
    "4. Select **\"Read\"** permission (sufficient for downloading models)\n",
    "5. Click **\"Generate token\"**\n",
    "6. **Copy the token** (you'll need it in the next step)\n",
    "\n",
    "**2.2 Add Token to Colab Secrets:**\n",
    "1. In Google Colab, click the **üîë key icon** on the left sidebar (Secrets)\n",
    "2. Click **\"+ Add new secret\"**\n",
    "3. In the **Name** field, enter: `HF_TOKEN`\n",
    "4. In the **Value** field, paste your Hugging Face token\n",
    "5. Toggle **ON** the switch for \"Notebook access\"\n",
    "6. Your secret is now saved!\n",
    "\n",
    "**2.3 Load the Token:**\n",
    "\n",
    "Run the cell below to load your authentication token from Colab secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646,
     "referenced_widgets": [
      "f9ae521caf0246dd9b21a19175c68b80",
      "baf63ea75ea14bc78ffb5b1a0efb4510",
      "58d727ad53fe4892891deb8b3d308616",
      "7f243b66cbfc4dd88acc3848b2ad8988",
      "09a30eed80354037aa681727b7a86539",
      "829575bc82e547f78a4d7bdbf1035105",
      "68e0f8623add410c9440bf73a03c85a7",
      "0adcabc2b1434969969108c191c0b3d8",
      "62270e7f8ff8421aad2f7d4515747121",
      "56c2fad8a62c421185048778b0d6a371",
      "feb3fc9d9d8c4c6aa6d0ebd4364ca356",
      "df03ad90706c4fa799ab79cff9b3b46d",
      "c00086e48ad1447988f8b05328de793a",
      "fca5b63612c1421b8afd63aeeef3c80c",
      "0ff90303b43c42ef89527a4add319722",
      "f1da4a65571748cea769b7b4d51bc490",
      "5ba036112ce64d7d8401150f12a26995",
      "37a4b1e91783400c97f01338d62e7810",
      "cfc3348cc5954016bfd546199c004717",
      "6286ca0c34bf4986a22d8ddfff5e05fd",
      "c70c824d1914470ab72789b067929acb",
      "d482642592cd4e678cf7f13d5fb22626",
      "0e227914f3914712bf1cdde639f13738",
      "7323f64f50974d47a2c9dde6e7c1bc71",
      "9fb13f9e25884cb4a4d4529d0c299392",
      "0cc630551ce444658b0d77e492dd4399",
      "ddcc136921864990b667fe2ce26b6f03",
      "941bce75f282411fbe4ac4e620cf09e7",
      "c7c31c2f5ad04438893d1f99b4bb24ac",
      "6d5a6ede05cf4f89846d8c2721718e18",
      "408c60c9df434ba8896bda1ae74ed095",
      "20ff32bd73b8414d8f707fef942360a2",
      "1113cf46247f4e7bbf0d76d51b358293",
      "43d02dbc42ed41079b39258e9ac7ff8d",
      "2aaba1ad33204905bea0f5b8a2133d98",
      "21e713d23217496292da5ad31340a3c2",
      "9d012fd1d5c3489d90056996c45b6e7f",
      "050f9c2737e54c35a3bae66a3397ba9c",
      "ad24cb4d3e8c40c1a0f0a53e3633843f",
      "93569859e7c045a7b71c58f48dabaa91",
      "2e7cf33e2806439b851e06cec1c572c7",
      "76df4017f7fe4ebf9fe8ad02af2ff0bd",
      "22bfa06d58e24180a6eaf4c0653d3327",
      "591aec805f5b436599b12afff7edf219",
      "218e3c23ff3c4efca295d3ddc0e40e5e",
      "efb2129891174bd68c7dfa1976c18a68",
      "d9f95fe2a0914ad491199032f2db88ee",
      "f7cf37bcfadd4c14b693d75dd18d3430",
      "ee627ef40bc94fa5a5a8a2ab3493e624",
      "5049cf49f6c34addb2201609dd12e879",
      "beb712af95724066a53351a7e2446784",
      "4823787d22234959a4f37a21ad109983",
      "e2c3c8d9c8204806a16d37cfec651ea3",
      "ac96aed66857436688ebc5f6b2b9ad86",
      "3a0ed72c8225458d92644a56d3779569",
      "3a54cfdce28b4dbd9dfb5a4fe3cbe48e",
      "6572dc8a99194a7d94673a134dfdd3d4",
      "a33b9d13e8af47d9ab5cb9f1b47832d4",
      "5eb52bd671d0441a8f8b14aaec65bae3",
      "0deb781746cd4ce0b6df7cd07c7a119e",
      "834fcb25343749e7994a26708ffef1c6",
      "036f8e6e48ae42e5b8f183527a8b2351",
      "a574f73615b04334b046f48ad16f4a39",
      "5fd99ad8c30947e68ca646c21dd1e2f4",
      "6635f13de3a744b7bc18a07d5090abfa",
      "87f17e805efe4bdc85d9ff1bce7887d0",
      "5e8d4a342ff54d57bb386f597c04c33a",
      "bf778a32e1f34817bdf22e1b39817f19",
      "5e43045bb2114820b91262da78ca286f",
      "ede1e0239ecf4f7faa761f74dfd44816",
      "99f4df24d7f440058a010fd6e3588428",
      "39a32321982a408d885c039a45b6e4a1",
      "9560a99892f74bf8ac9f1f19c4dd2d37",
      "1563c250780c4ac28048c755d08d9d00",
      "f2f61bf035f4430fae7a0dbb1a685f24",
      "e3429560210f40bf9698db4d48187518",
      "3a4bbf21cc304a7192b5cc0164ad94e9",
      "5523da7d0e7e45709e957e0d17be9930",
      "02882cd44e484a9b9941d009c00bbb88",
      "77863d6913ae4fbf8e7d9ece20f82b5a",
      "598cfe8e62a145b08ca926093bfaab53",
      "fa50d3ac7e9645fda65a4e2fdd459a1a",
      "42a589d3bad14a4f9590678d2d811e54",
      "1a511d5a4d854265bf5d86b60a8ef899",
      "8362a39414764baeb3033b950df8a6c3",
      "8e82c58567394933b038c058c7dea07e",
      "367c0134d75b4a718b4442a6129af100",
      "6118fc6172e24c3ca0ef2a8d61b30050",
      "a932280165d24b0fbdaee507b564e9cc",
      "a546cd1a4e9d40eeae3528120aa0bfe2",
      "b3e0ce356a284c18bf01eede3f33fbf1",
      "ac3a5fdeb80245a7a5909dad583e348c",
      "789f6451469a4124ad756dcd42ba0a8d",
      "fbdb7d4cd9154f54a0a8ca4ad9acc423",
      "f101a0cdaf854ee4a47d388a2c3a9a88",
      "be5e4cc2cfba4326ac3f1d2f7380c396",
      "a76901e646a14ed4b32f1c171657b97e",
      "a79bf77392484cd3a542e416ae63b0c7",
      "ab95a48c14184baab70339e772e56fbc",
      "d4e0ff4c4af54f80bb5ea3398b1f45e9",
      "5bb214956391486a8d1b063bc5eb4229",
      "856b8bcf12be48629cd316fa3edb8947",
      "157f9932c22d48d89d32e0fd652b39d1",
      "a8bdcf07c096494fbd0df8966a9d89e2",
      "62c31ff1950a483a90aa3089de63f93c",
      "1b90979ce5284be99ca93d452435640f",
      "ef71fd2763c5424089ac3c535eb95b39",
      "5df5d0d7695b4abeb272a1a7f56e66fd",
      "100d457fb8614fc2a3bac21986f072fe",
      "bd5bae142881447da38f1fbc93d2609b",
      "fce1f399516d451392d8cdf91d69b5af",
      "963b3fa9fbe944dd95eab77164e4b972",
      "dd0d9af66e7244dbabc514fe98918ea1",
      "7468746f78214a17904ca65bf496179a",
      "6633fa25c69c4f1ea7497f5eeb908d27",
      "bb1fe43bf7564839ae192b024caf44b3",
      "e7c7e3dfbf1b46898d8bbaff08f9c0c7",
      "ae9149a4a9a546278a8fe0adae202dab",
      "6b476f67fc804d2eb6355e84298ac743",
      "943db268a16642c7a20c1be96678f159",
      "3fb4e929cd984c1a91c75fe10f117995"
     ]
    },
    "id": "peHQzpRGuwTg",
    "outputId": "5e9ff8ed-7492-46dc-a814-92d6818dc50c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"üì• Loading Gemma 2 2B model...\")\n",
    "print(\"   This will download ~5GB on first run (cached for future use)\")\n",
    "print(\"   Please wait 1-2 minutes...\\n\")\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# Load tokenizer with authentication\n",
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "print(\"   ‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "print(\"\\nü§ñ Loading model with 4-bit quantization...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Quantization type (nf4 or fp4)\n",
    "    bnb_4bit_use_double_quant=True        # Nested quantization for better memory efficiency\n",
    ")\n",
    "\n",
    "# Load model with quantization config and authentication\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",              # Automatically place on GPU\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=HF_TOKEN                  # Pass authentication token\n",
    ")\n",
    "print(\"   ‚úÖ Model loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Gemma 2 2B is ready to use!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_UP8bkKuwTg"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Import MCP Server Functions\n",
    "\n",
    "We'll import functions directly from all five MCP servers (same approach as Notebook 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DgLK2E-uwTg",
    "outputId": "52cf2f98-dc9d-4b64-d3b1-f5421a8cce79"
   },
   "outputs": [],
   "source": [
    "# Ticket Server\n",
    "from ticket_server import (\n",
    "    search_tickets,\n",
    "    get_ticket_details,\n",
    "    get_ticket_metrics,\n",
    "    find_similar_tickets_to\n",
    ")\n",
    "\n",
    "# Customer Server\n",
    "from customer_server import (\n",
    "    lookup_customer,\n",
    "    check_customer_status,\n",
    "    get_sla_terms,\n",
    "    list_customer_contacts\n",
    ")\n",
    "\n",
    "# Billing Server\n",
    "from billing_server import (\n",
    "    get_invoice,\n",
    "    check_payment_status,\n",
    "    get_billing_history,\n",
    "    calculate_outstanding_balance\n",
    ")\n",
    "\n",
    "# Knowledge Base Server\n",
    "from kb_server import (\n",
    "    search_solutions,\n",
    "    get_article,\n",
    "    find_related_articles,\n",
    "    get_common_fixes\n",
    ")\n",
    "\n",
    "# Asset Server\n",
    "from asset_server import (\n",
    "    lookup_asset,\n",
    "    check_warranty,\n",
    "    get_asset_history\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All MCP server functions imported successfully!\")\n",
    "print(\"   Total: 20 tools available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzSSf8BAuwTh"
   },
   "source": [
    "### 2.1 Create Tool Registry\n",
    "\n",
    "We need to describe each tool in a format the model understands. Gemma 2 2B works well with OpenAI-style function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2NPIOKluwTh",
    "outputId": "5efc82a5-8b39-4d24-f227-fbf68c6871d7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define available tools in OpenAI function calling format\n",
    "# Most instruction-tuned models understand this standard format\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_tickets\",\n",
    "            \"description\": \"Search for support tickets with optional filters for status, priority, customer_id, or keywords\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"status\": {\"type\": \"string\", \"description\": \"Filter by status: open, in_progress, resolved, closed\"},\n",
    "                    \"priority\": {\"type\": \"string\", \"description\": \"Filter by priority: low, medium, high, critical\"},\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Filter by customer ID (e.g., CUST-001)\"},\n",
    "                    \"keyword\": {\"type\": \"string\", \"description\": \"Search in subject and description\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticket_details\",\n",
    "            \"description\": \"Get detailed information about a specific ticket by its ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticket_id\": {\"type\": \"string\", \"description\": \"Ticket ID (e.g., TKT-1001)\"}\n",
    "                },\n",
    "                \"required\": [\"ticket_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticket_metrics\",\n",
    "            \"description\": \"Get ticket statistics and metrics for a time period\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"period\": {\"type\": \"string\", \"description\": \"Time period: last_7_days, last_30_days, last_90_days\"}\n",
    "                },\n",
    "                \"required\": [\"period\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_customer\",\n",
    "            \"description\": \"Look up customer information by customer ID or company name\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID (e.g., CUST-001)\"},\n",
    "                    \"company_name\": {\"type\": \"string\", \"description\": \"Company name to search for\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_sla_terms\",\n",
    "            \"description\": \"Get SLA (Service Level Agreement) terms for a customer\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID (e.g., CUST-001)\"}\n",
    "                },\n",
    "                \"required\": [\"customer_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_invoice\",\n",
    "            \"description\": \"Get invoice information by invoice ID or customer ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"invoice_id\": {\"type\": \"string\", \"description\": \"Invoice ID (e.g., INV-1001)\"},\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID to get all invoices\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_outstanding_balance\",\n",
    "            \"description\": \"Calculate total outstanding and overdue balance for a customer\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID (e.g., CUST-001)\"}\n",
    "                },\n",
    "                \"required\": [\"customer_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_solutions\",\n",
    "            \"description\": \"Search knowledge base articles by keyword or topic\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "                    \"limit\": {\"type\": \"integer\", \"description\": \"Maximum number of results (default 5)\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_article\",\n",
    "            \"description\": \"Get full knowledge base article by article ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"article_id\": {\"type\": \"string\", \"description\": \"Article ID (e.g., KB-001)\"}\n",
    "                },\n",
    "                \"required\": [\"article_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_asset\",\n",
    "            \"description\": \"Look up asset information by asset ID\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"asset_id\": {\"type\": \"string\", \"description\": \"Asset ID (e.g., AST-SRV-001)\"}\n",
    "                },\n",
    "                \"required\": [\"asset_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"check_warranty\",\n",
    "            \"description\": \"Check warranty status and details for an asset\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"asset_id\": {\"type\": \"string\", \"description\": \"Asset ID (e.g., AST-SRV-001)\"}\n",
    "                },\n",
    "                \"required\": [\"asset_id\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a mapping from tool names to actual Python functions\n",
    "tool_map = {\n",
    "    \"search_tickets\": search_tickets,\n",
    "    \"get_ticket_details\": get_ticket_details,\n",
    "    \"get_ticket_metrics\": get_ticket_metrics,\n",
    "    \"lookup_customer\": lookup_customer,\n",
    "    \"get_sla_terms\": get_sla_terms,\n",
    "    \"get_invoice\": get_invoice,\n",
    "    \"calculate_outstanding_balance\": calculate_outstanding_balance,\n",
    "    \"search_solutions\": search_solutions,\n",
    "    \"get_article\": get_article,\n",
    "    \"lookup_asset\": lookup_asset,\n",
    "    \"check_warranty\": check_warranty\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Tool registry created with {len(tools)} tools\")\n",
    "print(\"\\nAvailable tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool['function']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBP3HN7ouwTh"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Tool Calling Implementation\n",
    "\n",
    "### 3.1 How It Works (Brief Review)\n",
    "\n",
    "You already know the tool calling loop from previous notebooks:\n",
    "\n",
    "1. **User asks a question** ‚Üí Model receives query + available tools\n",
    "2. **Model decides what to do** ‚Üí Either call tools OR give final answer\n",
    "3. **Execute tools** ‚Üí Run Python functions, get results\n",
    "4. **Feed results back** ‚Üí Model uses results to answer\n",
    "5. **Repeat if needed** ‚Üí Model might call more tools\n",
    "\n",
    "The key difference here: we're implementing this manually with a local model instead of using an API service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axjZeAP7uwTi"
   },
   "source": [
    "### 3.2 Helper Functions\n",
    "\n",
    "First, let's create helper functions for executing tools and generating model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyBbfDCIuwTi",
    "outputId": "4a8f72de-5feb-4f2c-ee33-b12ed33afe44"
   },
   "outputs": [],
   "source": [
    "def execute_tool(tool_name, arguments):\n",
    "    \"\"\"\n",
    "    Execute a tool by name with given arguments.\n",
    "\n",
    "    Args:\n",
    "        tool_name: Name of the tool to execute\n",
    "        arguments: Dictionary of arguments to pass\n",
    "\n",
    "    Returns:\n",
    "        Tool execution result (dict or error message)\n",
    "    \"\"\"\n",
    "    if tool_name not in tool_map:\n",
    "        return {\"error\": f\"Tool '{tool_name}' not found\"}\n",
    "\n",
    "    try:\n",
    "        # Call the actual Python function\n",
    "        result = tool_map[tool_name](**arguments)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Tool execution failed: {str(e)}\"}\n",
    "\n",
    "\n",
    "def generate_response(messages, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generate a response from the model given conversation history.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    # Format messages using the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode only the new tokens (not the input)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def parse_tool_calls(response):\n",
    "    \"\"\"\n",
    "    Parse tool calls from model response.\n",
    "    Different models format tool calls differently - this function tries to handle multiple formats.\n",
    "\n",
    "    Args:\n",
    "        response: Model's text response\n",
    "\n",
    "    Returns:\n",
    "        List of tool call dictionaries, or None if no tool calls\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try format 1: <tool_call> tags (used by some models like Hermes)\n",
    "        if \"<tool_call>\" in response:\n",
    "            start = response.find(\"<tool_call>\") + len(\"<tool_call>\")\n",
    "            end = response.find(\"</tool_call>\")\n",
    "            json_str = response[start:end].strip()\n",
    "\n",
    "            # Parse the JSON\n",
    "            tool_calls = json.loads(json_str)\n",
    "\n",
    "            # Ensure it's a list\n",
    "            if isinstance(tool_calls, dict):\n",
    "                tool_calls = [tool_calls]\n",
    "\n",
    "            return tool_calls\n",
    "\n",
    "        # Try format 2: Look for JSON blocks with \"name\" and \"arguments\" keys\n",
    "        # This is a fallback for models that output JSON directly\n",
    "        import re\n",
    "        # Find all JSON-like structures in the response\n",
    "        json_pattern = r'\\{[^{}]*\"name\"\\s*:\\s*\"[^\"]+\"\\s*,\\s*\"arguments\"\\s*:\\s*\\{[^}]*\\}[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, response, re.DOTALL)\n",
    "\n",
    "        if matches:\n",
    "            tool_calls = []\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    tool_call = json.loads(match)\n",
    "                    if \"name\" in tool_call:\n",
    "                        tool_calls.append(tool_call)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if tool_calls:\n",
    "                return tool_calls\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error parsing tool calls: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLkI9DVVuwTj"
   },
   "source": [
    "### 3.3 Main Query Function\n",
    "\n",
    "This implements the complete tool calling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8HaABQvzTSN",
    "outputId": "ed562d0d-ccef-434d-d273-a10321f3bbc7"
   },
   "outputs": [],
   "source": [
    "def query_with_tools(user_question, max_iterations=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Process a user query with tool calling capability.\n",
    "\n",
    "    Args:\n",
    "        user_question: Natural language question from user\n",
    "        max_iterations: Maximum number of tool calling rounds\n",
    "        verbose: Print intermediate steps\n",
    "\n",
    "    Returns:\n",
    "        Final answer from the model\n",
    "    \"\"\"\n",
    "    # Build combined message (Gemma doesn't support system role)\n",
    "    # We combine the system instructions with the user message\n",
    "    system_instructions = f\"\"\"You are a helpful IT support assistant with access to multiple tools.\n",
    "\n",
    "When you need information to answer a question, use the available tools by generating a tool call in this format:\n",
    "<tool_call>\n",
    "{{\n",
    "  \"name\": \"tool_name\",\n",
    "  \"arguments\": {{\n",
    "    \"param1\": \"value1\",\n",
    "    \"param2\": \"value2\"\n",
    "  }}\n",
    "}}\n",
    "</tool_call>\n",
    "\n",
    "You can call multiple tools by providing a list of tool calls.\n",
    "\n",
    "Available tools:\n",
    "{json.dumps([t['function'] for t in tools], indent=2)}\n",
    "\n",
    "After receiving tool results, use them to provide a helpful answer to the user.\n",
    "\n",
    "---\n",
    "\n",
    "User question: {user_question}\"\"\"\n",
    "\n",
    "    # Initialize conversation (no system role for Gemma)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": system_instructions}\n",
    "    ]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"üë§ User: {user_question}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "    # Tool calling loop\n",
    "    for iteration in range(max_iterations):\n",
    "        if verbose:\n",
    "            print(f\"\\nüîÑ Iteration {iteration + 1}\")\n",
    "\n",
    "        # Get model response\n",
    "        response = generate_response(messages)\n",
    "\n",
    "        # Check if model wants to call tools\n",
    "        tool_calls = parse_tool_calls(response)\n",
    "\n",
    "        if tool_calls:\n",
    "            # Model wants to use tools\n",
    "            if verbose:\n",
    "                print(f\"üîß Model requested {len(tool_calls)} tool call(s)\")\n",
    "\n",
    "            # Add assistant's response to history\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "            # Execute each tool\n",
    "            tool_results = []\n",
    "            for tool_call in tool_calls:\n",
    "                tool_name = tool_call.get(\"name\")\n",
    "                arguments = tool_call.get(\"arguments\", {})\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"\\n  Calling: {tool_name}\")\n",
    "                    print(f\"  Args: {json.dumps(arguments, indent=4)}\")\n",
    "\n",
    "                # Execute the tool\n",
    "                result = execute_tool(tool_name, arguments)\n",
    "                tool_results.append(result)\n",
    "\n",
    "                if verbose:\n",
    "                    result_str = json.dumps(result, indent=4)\n",
    "                    preview = result_str[:200] + \"...\" if len(result_str) > 200 else result_str\n",
    "                    print(f\"  Result: {preview}\")\n",
    "\n",
    "            # Add tool results to conversation\n",
    "            tool_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Tool results:\\n{json.dumps(tool_results, indent=2)}\"\n",
    "            }\n",
    "            messages.append(tool_message)\n",
    "\n",
    "            # Continue loop to get next response\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # No tool calls - this is the final answer\n",
    "            if verbose:\n",
    "                print(\"\\n‚úÖ Final answer received\")\n",
    "                print(\"\\n\" + \"=\" * 70)\n",
    "                print(f\"ü§ñ Assistant: {response}\")\n",
    "                print(\"=\" * 70)\n",
    "\n",
    "            return response\n",
    "\n",
    "    # Hit max iterations\n",
    "    return \"I've reached the maximum number of tool calls. Please try simplifying your question.\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Main query function updated for Gemma compatibility\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a45O3ASxuwTj"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Examples & Demonstrations\n",
    "\n",
    "Now let's test the system with various queries!\n",
    "\n",
    "### Example 1: Simple Single-Tool Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhUyQQVpuwTj",
    "outputId": "96066df9-3ad2-438e-86a9-81224da7936d"
   },
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"What are all the critical priority tickets?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T5CT5KwuwTj"
   },
   "source": [
    "**What happened:**\n",
    "1. Model analyzed the question\n",
    "2. Decided to call `search_tickets` with `priority=\"critical\"`\n",
    "3. Received the results\n",
    "4. Formatted a natural language response\n",
    "\n",
    "### Example 2: Multi-Tool Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSKvNLpluwTj",
    "outputId": "68a454ea-254d-418d-b607-03f0e40a67dd"
   },
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Show me customer CUST-001's information and their SLA terms\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo-j8KMJuwTj"
   },
   "source": [
    "**What happened:**\n",
    "- Model called TWO tools: `lookup_customer` AND `get_sla_terms`\n",
    "- Combined the information into a coherent answer\n",
    "\n",
    "### Example 3: Complex Multi-Step Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjSf3k72uwTj",
    "outputId": "a365be42-506d-4c4e-b222-815b13142667"
   },
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Which customers have critical tickets? Show their outstanding balances.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL0k2XQKuwTj"
   },
   "source": [
    "**What happened:**\n",
    "1. First tool call: `search_tickets(priority=\"critical\")` to find critical tickets\n",
    "2. Extracted customer IDs from ticket results\n",
    "3. Multiple tool calls: `calculate_outstanding_balance` for each customer\n",
    "4. Correlated the data and formatted the answer\n",
    "\n",
    "This demonstrates **multi-step reasoning** - the model chains tools together!\n",
    "\n",
    "### Example 4: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5BrA65zuwTk",
    "outputId": "fa216ecc-e040-4cf0-84e7-c8bf44a21415"
   },
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"Get details for ticket TKT-9999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59V-bEUvuwTk"
   },
   "source": [
    "**What happened:**\n",
    "- Tool returned an error (ticket not found)\n",
    "- Error included helpful hints (from your MCP server design!)\n",
    "- Model used the hints to provide a helpful response to the user\n",
    "\n",
    "### Example 5: Knowledge Base Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-Aq1fYfuwTk",
    "outputId": "c4e6ca04-f952-4621-baac-ba357515906c"
   },
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Find articles about Windows blue screen errors and show me the most relevant one\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82SWKG0UuwTk"
   },
   "source": [
    "### Example 6: Asset and Warranty Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLJ4GYQNuwTk",
    "outputId": "8e7699a7-835a-4b96-d0a3-2fcd2d9cf8e3"
   },
   "outputs": [],
   "source": [
    "answer = query_with_tools(\n",
    "    \"Check the warranty status for asset AST-SRV-001. Is it still valid?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAhbqDIPuwTk"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Try It Yourself!\n",
    "\n",
    "Now it's your turn. Use the `query_with_tools()` function to ask your own questions.\n",
    "\n",
    "### Your Custom Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDPyA6n9uwTk",
    "outputId": "038412e4-6a1a-4d34-c08e-4f48c4c7afcd"
   },
   "outputs": [],
   "source": [
    "# Write your own query here!\n",
    "my_query = \"Your question here...\"\n",
    "\n",
    "answer = query_with_tools(my_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6JjcLHLuwTk"
   },
   "source": [
    "### Suggested Queries to Try\n",
    "\n",
    "```python\n",
    "# Cross-server correlation\n",
    "query_with_tools(\"Show me all high priority tickets for customers with overdue invoices\")\n",
    "\n",
    "# Metrics and analytics\n",
    "query_with_tools(\"What are the ticket metrics for the last 30 days?\")\n",
    "\n",
    "# Customer insights\n",
    "query_with_tools(\"Tell me about customer CUST-002, their tickets, and billing status\")\n",
    "\n",
    "# Knowledge base assistance\n",
    "query_with_tools(\"What are common fixes for network connectivity issues?\")\n",
    "\n",
    "# Asset management\n",
    "query_with_tools(\"Which assets have expired warranties?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWs8cGMduwTk"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Understanding Local vs API Models\n",
    "\n",
    "### Key Differences You Experienced\n",
    "\n",
    "| Aspect | Local Model (Gemma 2 2B) | API Model (OpenAI) |\n",
    "|--------|---------------------------|--------------------|\n",
    "| **Privacy** | ‚úÖ All data stays local | ‚ö†Ô∏è Sent to external servers |\n",
    "| **Cost** | ‚úÖ Free (after setup) | üí∞ Pay per token |\n",
    "| **Speed** | ‚úÖ Fast (especially 2B models) | ‚úÖ Fast (optimized infrastructure) |\n",
    "| **Quality** | ‚ö†Ô∏è Good for simple tasks, may struggle with complex reasoning | ‚úÖ Excellent reasoning |\n",
    "| **Offline** | ‚úÖ Works without internet | ‚ùå Requires connection |\n",
    "| **Setup** | ‚ö†Ô∏è More complex | ‚úÖ Simple API key |\n",
    "| **Customization** | ‚úÖ Can fine-tune, modify | ‚ùå Limited control |\n",
    "| **Memory** | ‚úÖ 2-3GB VRAM (Colab friendly!) | N/A (runs on their servers) |\n",
    "\n",
    "### When to Use Local Models\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Sensitive data (healthcare, finance, legal)\n",
    "- High-volume queries (cost savings)\n",
    "- Offline/air-gapped environments\n",
    "- Learning and experimentation\n",
    "- Custom fine-tuning needs\n",
    "- Limited GPU resources (2B models are very efficient!)\n",
    "\n",
    "‚ö†Ô∏è **Consider APIs when:**\n",
    "- Need highest quality reasoning\n",
    "- Complex multi-step tasks\n",
    "- Want minimal setup complexity\n",
    "- Need consistent high-quality responses\n",
    "\n",
    "### Potential Limitations You Might Notice\n",
    "\n",
    "1. **Tool Selection Errors**: Smaller models (2B) may occasionally:\n",
    "   - Choose wrong tool\n",
    "   - Hallucinate tool names\n",
    "   - Miss required parameters\n",
    "   - Need more explicit instructions\n",
    "\n",
    "2. **Response Quality**: May be less natural or detailed than larger models\n",
    "\n",
    "3. **Complex Reasoning**: Multi-step queries might require more guidance or simpler prompts\n",
    "\n",
    "4. **Consistency**: May vary more in quality compared to larger models\n",
    "\n",
    "**Note:** These trade-offs are why hybrid approaches exist - use local for routine queries, API for complex cases. Gemma 2 2B is particularly good for learning and simple tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uGJduhMuwTk"
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced Exercises\n",
    "\n",
    "### Exercise 1: Add Conversation History\n",
    "\n",
    "Modify `query_with_tools()` to support multi-turn conversations:\n",
    "```python\n",
    "# First query\n",
    "history = []\n",
    "answer1, history = query_with_tools_stateful(\"Show me critical tickets\", history)\n",
    "\n",
    "# Follow-up query (model remembers context)\n",
    "answer2, history = query_with_tools_stateful(\"What are their customer IDs?\", history)\n",
    "```\n",
    "\n",
    "### Exercise 2: Implement Confidence Scoring\n",
    "\n",
    "Add logic to detect when the model is uncertain and ask for clarification:\n",
    "- Parse for uncertainty phrases (\"I'm not sure\", \"might be\", etc.)\n",
    "- Request confirmation before executing potentially wrong tools\n",
    "\n",
    "### Exercise 3: Tool Call Validation\n",
    "\n",
    "Add a validation layer:\n",
    "- Check if tool exists before calling\n",
    "- Validate parameter types match expected schema\n",
    "- Provide better error messages to model when validation fails\n",
    "\n",
    "### Exercise 4: Compare Different Models\n",
    "\n",
    "Try swapping in a different model to compare:\n",
    "- **Gemma 2 9B** (`google/gemma-2-9b-it`) - Same family, better quality\n",
    "- **Qwen2.5-7B-Instruct** - Good alternative in the 7B range\n",
    "- **Phi-3-mini** (`microsoft/Phi-3-mini-4k-instruct`) - Another efficient small model\n",
    "- **Llama-3.2-3B** - Meta's small instruction model\n",
    "\n",
    "Compare:\n",
    "- Tool selection accuracy\n",
    "- Response quality\n",
    "- Speed\n",
    "- Memory usage\n",
    "\n",
    "**Tip**: Just change the `model_name` variable in cell 6!\n",
    "\n",
    "### Exercise 5: Performance Optimization\n",
    "\n",
    "Optimize for speed:\n",
    "- Reduce `max_new_tokens` for tool calls (tool calls don't need long responses)\n",
    "- Implement caching for repeated queries\n",
    "- Batch multiple tool calls when possible\n",
    "- Try removing quantization if you have enough VRAM (faster inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgz7fdluwTl"
   },
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "‚úÖ **Local Model Setup**\n",
    "   - Loading Gemma 2 2B with 4-bit quantization\n",
    "   - GPU optimization for inference\n",
    "   - Memory management in Colab (only 2-3GB VRAM needed!)\n",
    "   - Fast loading and inference times\n",
    "\n",
    "‚úÖ **Tool Calling Implementation**\n",
    "   - Creating tool schemas in OpenAI function format\n",
    "   - Parsing model outputs for tool calls\n",
    "   - Executing Python functions dynamically\n",
    "   - Multi-turn tool calling loops\n",
    "   - Handling different model output formats\n",
    "\n",
    "‚úÖ **Practical Applications**\n",
    "   - Privacy-preserving AI for sensitive data\n",
    "   - Cost-effective alternative to API services\n",
    "   - Understanding tool calling mechanics\n",
    "   - Running on free-tier Google Colab\n",
    "\n",
    "‚úÖ **Trade-offs and Decisions**\n",
    "   - When to use local vs API models\n",
    "   - Performance vs quality considerations\n",
    "   - Memory efficiency with smaller models\n",
    "   - Real-world deployment scenarios\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Privacy Matters**: Local models enable AI on sensitive data without external dependencies\n",
    "\n",
    "2. **Tool Calling is Universal**: Same concepts work across OpenAI, Anthropic, and open source models\n",
    "\n",
    "3. **Size vs Performance**: Smaller models (2B) can handle tool calling with the right setup, though with some quality trade-offs\n",
    "\n",
    "4. **Open Source = Control**: Full transparency and customization options\n",
    "\n",
    "5. **Implementation Matters**: Prompt engineering and error handling crucial for success\n",
    "\n",
    "6. **Efficiency First**: Gemma 2 2B proves you don't always need large models - 2B parameters can be surprisingly capable!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Fine-tune Gemma** on your specific tools/domain for better accuracy\n",
    "- **Deploy in production** with proper error handling and monitoring  \n",
    "- **Explore other models** (try Gemma 2 9B for better quality, or Qwen 2.5 for comparison)\n",
    "- **Build hybrid systems** (local for routine queries, API for complex cases)\n",
    "- **Optimize prompts** to get better tool selection from smaller models\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You now know how to build lightweight, privacy-preserving AI assistants with tool calling capabilities using small open source models that run perfectly on Google Colab!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
