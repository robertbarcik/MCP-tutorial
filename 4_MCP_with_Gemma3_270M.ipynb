{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP with Ultra-Lightweight Models - Gemma 3 270M\n",
    "\n",
    "This notebook demonstrates how to use MCP servers with **Gemma 3 270M**, an ultra-lightweight language model that runs even on CPU!\n",
    "\n",
    "## Why Use Ultra-Lightweight Models?\n",
    "\n",
    "‚úÖ **Minimal Resources** - Runs on CPU or tiny GPUs  \n",
    "‚úÖ **Blazing Fast** - Almost instant responses  \n",
    "‚úÖ **Privacy** - Company data never leaves your machine  \n",
    "‚úÖ **Cost** - No API fees, minimal compute requirements  \n",
    "‚úÖ **Learning** - Perfect for understanding tool calling mechanics  \n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Load and run **Gemma 3 270M** locally (ultra-lightweight!)\n",
    "2. Connect the model to MCP server functions\n",
    "3. Implement tool calling from scratch\n",
    "4. Process natural language queries end-to-end\n",
    "5. Understand trade-offs of tiny models\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- GPU optional (works on CPU too!)\n",
    "- ~1GB RAM for model with quantization\n",
    "- Python 3.10+\n",
    "\n",
    "‚ö†Ô∏è **Note**: Being only 270M parameters, this model is best for:\n",
    "- Simple, straightforward tasks\n",
    "- Learning and experimentation\n",
    "- Environments with very limited resources\n",
    "- Demonstrating concepts\n",
    "\n",
    "For production use or complex reasoning, consider larger models like Gemma 2 2B or 9B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Model Loading\n",
    "\n",
    "### 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q transformers torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"   üí° Running on CPU - Gemma 3 270M is small enough to run on CPU!\")\n",
    "    print(\"   Inference will be slower but still usable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 About Gemma 3 270M\n",
    "\n",
    "**Why this model?**  \n",
    "Gemma 3 270M is Google's ultra-lightweight **base model** that's perfect for resource-constrained environments. It's specifically designed for:\n",
    "- **Extreme efficiency** - Only 270 million parameters\n",
    "- **CPU-friendly** - Can run on CPU without GPU\n",
    "- **Fast inference** - Near-instant responses even on modest hardware\n",
    "- **32K context window** - Large context for its size\n",
    "- **Learning-focused** - Great for understanding AI concepts\n",
    "\n",
    "**Note:** First run will download ~1-2GB (cached for future use). With 4-bit quantization, it uses less than 1GB RAM!\n",
    "\n",
    "‚ö†Ô∏è **Important Distinction**: \n",
    "- This is a **base model** (not instruction-tuned like \"gemma-2-2b-it\")\n",
    "- Base models are less aligned for chat/instruction following\n",
    "- We use simple prompt formatting instead of chat templates\n",
    "- Expect lower quality than instruction-tuned models\n",
    "\n",
    "‚ö†Ô∏è **Important**: Gemma models require authentication - follow the steps in the next section.\n",
    "\n",
    "‚ö†Ô∏è **Performance Expectations**:\n",
    "- ‚úÖ Can demonstrate tool calling concepts\n",
    "- ‚ö†Ô∏è May struggle even with simple queries\n",
    "- ‚ö†Ô∏è Less reliable than instruction-tuned models\n",
    "- ‚ö†Ô∏è Best for learning, not production use\n",
    "- üí° **For better results, use `google/gemma-2-2b-it` (instruction-tuned version)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Authentication Setup (Required for Gemma Models)\n",
    "\n",
    "‚ö†Ô∏è **Important**: Gemma models are gated and require authentication. Follow these steps **before** running the next cell:\n",
    "\n",
    "#### Step 1: Accept the Gemma License\n",
    "\n",
    "1. Go to https://huggingface.co/google/gemma-3-270m\n",
    "2. If you don't have a Hugging Face account, click **Sign Up** (it's free)\n",
    "3. Log in to your account\n",
    "4. Click the **\"Agree and access repository\"** button to accept the license terms\n",
    "5. Wait for approval (usually instant, but may take 1-2 minutes)\n",
    "\n",
    "#### Step 2: Set Up Authentication in Google Colab\n",
    "\n",
    "Now you need to authenticate using a Hugging Face token:\n",
    "\n",
    "**2.1 Create a Hugging Face Token:**\n",
    "1. Go to https://huggingface.co/settings/tokens\n",
    "2. Click **\"New token\"**\n",
    "3. Give it a name (e.g., \"colab-access\")\n",
    "4. Select **\"Read\"** permission (sufficient for downloading models)\n",
    "5. Click **\"Generate token\"**\n",
    "6. **Copy the token** (you'll need it in the next step)\n",
    "\n",
    "**2.2 Add Token to Colab Secrets:**\n",
    "1. In Google Colab, click the **üîë key icon** on the left sidebar (Secrets)\n",
    "2. Click **\"+ Add new secret\"**\n",
    "3. In the **Name** field, enter: `HF_TOKEN`\n",
    "4. In the **Value** field, paste your Hugging Face token\n",
    "5. Toggle **ON** the switch for \"Notebook access\"\n",
    "6. Your secret is now saved!\n",
    "\n",
    "**2.3 Load the Token:**\n",
    "\n",
    "Run the cell below to load your authentication token from Colab secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face token from Colab secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úÖ Hugging Face token loaded successfully from Colab secrets!\")\n",
    "    print(f\"   Token preview: {HF_TOKEN[:10]}...\" if HF_TOKEN else \"   ‚ö†Ô∏è Token is empty!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error loading token from Colab secrets!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"\\nüí° Make sure you:\")\n",
    "    print(\"   1. Created a Hugging Face token at https://huggingface.co/settings/tokens\")\n",
    "    print(\"   2. Added it to Colab secrets with name 'HF_TOKEN'\")\n",
    "    print(\"   3. Enabled 'Notebook access' for the secret\")\n",
    "    HF_TOKEN = None\n",
    "\n",
    "# Verify token is available\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"‚ùå No HF_TOKEN found! Please follow the authentication steps above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Load Gemma 3 270M Model\n",
    "\n",
    "Now that you're authenticated, let's load the ultra-lightweight model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"üì• Loading Gemma 3 270M model...\")\n",
    "print(\"   This will download ~1-2GB on first run (cached for future use)\")\n",
    "print(\"   Please wait 30-60 seconds...\\n\")\n",
    "\n",
    "model_name = \"google/gemma-3-270m\"\n",
    "\n",
    "# Load tokenizer with authentication\n",
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "\n",
    "# Set padding token (required for generation)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"   ‚úÖ Tokenizer loaded\")\n",
    "print(\"   üí° Note: Gemma 3 270M is a base model (not instruction-tuned)\")\n",
    "print(\"   üí° We'll use simple prompt formatting instead of chat templates\")\n",
    "\n",
    "# Configure 4-bit quantization (optional for 270M, but saves even more memory)\n",
    "print(\"\\nü§ñ Loading model with 4-bit quantization...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Quantization type (nf4 or fp4)\n",
    "    bnb_4bit_use_double_quant=True        # Nested quantization for better memory efficiency\n",
    ")\n",
    "\n",
    "# Load model with quantization config and authentication\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",              # Automatically place on GPU or CPU\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=HF_TOKEN                  # Pass authentication token\n",
    ")\n",
    "print(\"   ‚úÖ Model loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Gemma 3 270M is ready to use!\")\n",
    "print(\"   Model size: 270 million parameters\")\n",
    "print(\"   Memory usage: <1GB with quantization\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Import MCP Server Functions\n",
    "\n",
    "We'll import functions directly from all five MCP servers (same approach as other notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket Server\n",
    "from ticket_server import (\n",
    "    search_tickets,\n",
    "    get_ticket_details,\n",
    "    get_ticket_metrics,\n",
    "    find_similar_tickets_to\n",
    ")\n",
    "\n",
    "# Customer Server\n",
    "from customer_server import (\n",
    "    lookup_customer,\n",
    "    check_customer_status,\n",
    "    get_sla_terms,\n",
    "    list_customer_contacts\n",
    ")\n",
    "\n",
    "# Billing Server\n",
    "from billing_server import (\n",
    "    get_invoice,\n",
    "    check_payment_status,\n",
    "    get_billing_history,\n",
    "    calculate_outstanding_balance\n",
    ")\n",
    "\n",
    "# Knowledge Base Server\n",
    "from kb_server import (\n",
    "    search_solutions,\n",
    "    get_article,\n",
    "    find_related_articles,\n",
    "    get_common_fixes\n",
    ")\n",
    "\n",
    "# Asset Server\n",
    "from asset_server import (\n",
    "    lookup_asset,\n",
    "    check_warranty,\n",
    "    get_asset_history\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All MCP server functions imported successfully!\")\n",
    "print(\"   Total: 11 core tools available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Tool Registry\n",
    "\n",
    "We'll use a **simplified tool set** for the 270M model to avoid overwhelming it. We focus on the most essential tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a simplified set of tools for the small model\n",
    "# Using fewer tools helps the 270M model make better decisions\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_tickets\",\n",
    "            \"description\": \"Search for support tickets by priority or status\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"priority\": {\"type\": \"string\", \"description\": \"Filter by priority: low, medium, high, critical\"},\n",
    "                    \"status\": {\"type\": \"string\", \"description\": \"Filter by status: open, in_progress, resolved\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticket_details\",\n",
    "            \"description\": \"Get details about a specific ticket\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticket_id\": {\"type\": \"string\", \"description\": \"Ticket ID like TKT-1001\"}\n",
    "                },\n",
    "                \"required\": [\"ticket_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_customer\",\n",
    "            \"description\": \"Look up customer information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_id\": {\"type\": \"string\", \"description\": \"Customer ID like CUST-001\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"check_warranty\",\n",
    "            \"description\": \"Check warranty status for an asset\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"asset_id\": {\"type\": \"string\", \"description\": \"Asset ID like AST-SRV-001\"}\n",
    "                },\n",
    "                \"required\": [\"asset_id\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a mapping from tool names to actual Python functions\n",
    "tool_map = {\n",
    "    \"search_tickets\": search_tickets,\n",
    "    \"get_ticket_details\": get_ticket_details,\n",
    "    \"lookup_customer\": lookup_customer,\n",
    "    \"check_warranty\": check_warranty\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Simplified tool registry created with {len(tools)} tools\")\n",
    "print(\"\\nüí° Note: Using fewer tools to help the 270M model make better decisions\")\n",
    "print(\"\\nAvailable tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool['function']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Tool Calling Implementation\n",
    "\n",
    "### 3.1 How It Works\n",
    "\n",
    "The tool calling loop:\n",
    "\n",
    "1. **User asks a question** ‚Üí Model receives query + available tools\n",
    "2. **Model decides what to do** ‚Üí Either call tools OR give final answer\n",
    "3. **Execute tools** ‚Üí Run Python functions, get results\n",
    "4. **Feed results back** ‚Üí Model uses results to answer\n",
    "5. **Repeat if needed** ‚Üí Model might call more tools\n",
    "\n",
    "For a 270M model, we keep prompts simpler and more direct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool(tool_name, arguments):\n",
    "    \"\"\"\n",
    "    Execute a tool by name with given arguments.\n",
    "    \n",
    "    Args:\n",
    "        tool_name: Name of the tool to execute\n",
    "        arguments: Dictionary of arguments to pass\n",
    "        \n",
    "    Returns:\n",
    "        Tool execution result (dict or error message)\n",
    "    \"\"\"\n",
    "    if tool_name not in tool_map:\n",
    "        return {\"error\": f\"Tool '{tool_name}' not found\"}\n",
    "    \n",
    "    try:\n",
    "        # Call the actual Python function\n",
    "        result = tool_map[tool_name](**arguments)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Tool execution failed: {str(e)}\"}\n",
    "\n",
    "\n",
    "def format_conversation(messages):\n",
    "    \"\"\"\n",
    "    Manually format conversation messages into a simple prompt.\n",
    "    Since Gemma 3 270M is a base model, we use simple formatting.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt_parts = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        role = msg['role']\n",
    "        content = msg['content']\n",
    "        \n",
    "        if role == 'user':\n",
    "            prompt_parts.append(f\"User: {content}\")\n",
    "        elif role == 'assistant':\n",
    "            prompt_parts.append(f\"Assistant: {content}\")\n",
    "    \n",
    "    # Add final assistant prompt\n",
    "    prompt_parts.append(\"Assistant:\")\n",
    "    \n",
    "    return \"\\n\\n\".join(prompt_parts)\n",
    "\n",
    "\n",
    "def generate_response(messages, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generate a response from the model given conversation history.\n",
    "    Uses greedy decoding to avoid sampling issues with small models.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        max_new_tokens: Maximum tokens to generate (reduced for 270M)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    # Manually format the conversation (no chat template for base models)\n",
    "    prompt = format_conversation(messages)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "    \n",
    "    # Generate using GREEDY DECODING (no sampling) to avoid NaN/Inf issues\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Use greedy decoding instead of sampling\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,  # Reduce repetition\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens (not the input)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def parse_tool_calls(response):\n",
    "    \"\"\"\n",
    "    Parse tool calls from model response.\n",
    "    \n",
    "    Args:\n",
    "        response: Model's text response\n",
    "        \n",
    "    Returns:\n",
    "        List of tool call dictionaries, or None if no tool calls\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try format 1: <tool_call> tags\n",
    "        if \"<tool_call>\" in response:\n",
    "            start = response.find(\"<tool_call>\") + len(\"<tool_call>\")\n",
    "            end = response.find(\"</tool_call>\")\n",
    "            json_str = response[start:end].strip()\n",
    "            \n",
    "            # Parse the JSON\n",
    "            tool_calls = json.loads(json_str)\n",
    "            \n",
    "            # Ensure it's a list\n",
    "            if isinstance(tool_calls, dict):\n",
    "                tool_calls = [tool_calls]\n",
    "            \n",
    "            return tool_calls\n",
    "        \n",
    "        # Try format 2: Look for JSON blocks with \"name\" and \"arguments\" keys\n",
    "        import re\n",
    "        json_pattern = r'\\{[^{}]*\"name\"\\s*:\\s*\"[^\"]+\"\\s*,\\s*\"arguments\"\\s*:\\s*\\{[^}]*\\}[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, response, re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            tool_calls = []\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    tool_call = json.loads(match)\n",
    "                    if \"name\" in tool_call:\n",
    "                        tool_calls.append(tool_call)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if tool_calls:\n",
    "                return tool_calls\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error parsing tool calls: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Helper functions defined (using greedy decoding for stability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Main Query Function (Optimized for Small Models)\n",
    "\n",
    "This version uses simpler prompts and instructions optimized for the 270M model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_tools(user_question, max_iterations=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Process a user query with tool calling capability.\n",
    "    Optimized for small base models like Gemma 3 270M.\n",
    "    \n",
    "    Args:\n",
    "        user_question: Natural language question from user\n",
    "        max_iterations: Maximum number of tool calling rounds (reduced for 270M)\n",
    "        verbose: Print intermediate steps\n",
    "        \n",
    "    Returns:\n",
    "        Final answer from the model\n",
    "    \"\"\"\n",
    "    # Build very simple, direct instructions for the base model\n",
    "    # Base models need simpler formatting than instruction-tuned models\n",
    "    system_instructions = f\"\"\"You are an IT support assistant. Available tools:\n",
    "\n",
    "{json.dumps([t['function'] for t in tools], indent=2)}\n",
    "\n",
    "To call a tool, output exactly:\n",
    "<tool_call>\n",
    "{{\"name\": \"tool_name\", \"arguments\": {{\"param\": \"value\"}}}}\n",
    "</tool_call>\n",
    "\n",
    "User question: {user_question}\"\"\"\n",
    "    \n",
    "    # Initialize conversation with simple user/assistant format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": system_instructions}\n",
    "    ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"üë§ User: {user_question}\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # Tool calling loop\n",
    "    for iteration in range(max_iterations):\n",
    "        if verbose:\n",
    "            print(f\"\\nüîÑ Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Get model response\n",
    "        response = generate_response(messages, max_new_tokens=200)\n",
    "        \n",
    "        # Check if model wants to call tools\n",
    "        tool_calls = parse_tool_calls(response)\n",
    "        \n",
    "        if tool_calls:\n",
    "            # Model wants to use tools\n",
    "            if verbose:\n",
    "                print(f\"üîß Model requested {len(tool_calls)} tool call(s)\")\n",
    "            \n",
    "            # Add assistant's response to history\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            # Execute each tool\n",
    "            tool_results = []\n",
    "            for tool_call in tool_calls:\n",
    "                tool_name = tool_call.get(\"name\")\n",
    "                arguments = tool_call.get(\"arguments\", {})\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"\\n  Calling: {tool_name}\")\n",
    "                    print(f\"  Args: {json.dumps(arguments, indent=4)}\")\n",
    "                \n",
    "                # Execute the tool\n",
    "                result = execute_tool(tool_name, arguments)\n",
    "                tool_results.append(result)\n",
    "                \n",
    "                if verbose:\n",
    "                    result_str = json.dumps(result, indent=4)\n",
    "                    preview = result_str[:200] + \"...\" if len(result_str) > 200 else result_str\n",
    "                    print(f\"  Result: {preview}\")\n",
    "            \n",
    "            # Add tool results to conversation with simple formatting\n",
    "            tool_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Tool results:\\n{json.dumps(tool_results, indent=2)}\\n\\nPlease provide your answer based on these results.\"\n",
    "            }\n",
    "            messages.append(tool_message)\n",
    "            \n",
    "            # Continue loop to get next response\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            # No tool calls - this is the final answer\n",
    "            if verbose:\n",
    "                print(\"\\n‚úÖ Final answer received\")\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(f\"ü§ñ Assistant: {response}\")\n",
    "                print(\"=\"*70)\n",
    "            \n",
    "            return response\n",
    "    \n",
    "    # Hit max iterations\n",
    "    return \"Maximum iterations reached. Please try a simpler question.\"\n",
    "\n",
    "print(\"‚úÖ Main query function defined (optimized for Gemma 3 270M base model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Examples & Demonstrations\n",
    "\n",
    "Let's test with **simple, focused queries** that work well with the 270M model!\n",
    "\n",
    "### Example 1: Simple Single-Tool Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"Show me critical priority tickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Get Ticket Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"Get details for ticket TKT-1001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Customer Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"Look up customer CUST-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Warranty Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_with_tools(\"Check warranty for asset AST-SRV-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Try It Yourself!\n",
    "\n",
    "### Tips for Best Results with 270M Model:\n",
    "\n",
    "‚úÖ **Do:**\n",
    "- Use simple, direct questions\n",
    "- Ask for one thing at a time\n",
    "- Be specific with IDs (e.g., \"TKT-1001\", \"CUST-002\")\n",
    "- Use short queries\n",
    "\n",
    "‚ö†Ô∏è **Avoid:**\n",
    "- Complex multi-step questions\n",
    "- Vague or ambiguous queries\n",
    "- Asking for analysis or reasoning\n",
    "- Long conversational prompts\n",
    "\n",
    "### Your Custom Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own simple query here!\n",
    "my_query = \"Show me open tickets\"\n",
    "\n",
    "answer = query_with_tools(my_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Simple Queries\n",
    "\n",
    "```python\n",
    "# Good queries for 270M:\n",
    "query_with_tools(\"Find high priority tickets\")\n",
    "query_with_tools(\"Get ticket TKT-1002 details\")\n",
    "query_with_tools(\"Look up customer CUST-003\")\n",
    "query_with_tools(\"Check warranty AST-SRV-002\")\n",
    "query_with_tools(\"Show open tickets\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Understanding Ultra-Lightweight Models\n",
    "\n",
    "### Gemma 3 270M vs Larger Models\n",
    "\n",
    "| Aspect | Gemma 3 270M | Gemma 2 2B | GPT-4 |\n",
    "|--------|--------------|------------|-------|\n",
    "| **Parameters** | 270M | 2B | >1 trillion |\n",
    "| **VRAM** | <1GB | 2-3GB | N/A (API) |\n",
    "| **Speed** | ‚ö°‚ö°‚ö° Blazing fast | ‚ö°‚ö° Fast | ‚ö° Fast |\n",
    "| **CPU Viable** | ‚úÖ Yes | ‚ö†Ô∏è Slow | ‚ùå No |\n",
    "| **Simple Tasks** | ‚úÖ Good | ‚úÖ‚úÖ Very good | ‚úÖ‚úÖ‚úÖ Excellent |\n",
    "| **Complex Reasoning** | ‚ùå Limited | ‚ö†Ô∏è Good | ‚úÖ‚úÖ‚úÖ Excellent |\n",
    "| **Cost** | ‚úÖ Free | ‚úÖ Free | üí∞üí∞ Expensive |\n",
    "| **Privacy** | ‚úÖ 100% local | ‚úÖ 100% local | ‚ùå Cloud |\n",
    "\n",
    "### When to Use 270M Models\n",
    "\n",
    "‚úÖ **Perfect for:**\n",
    "- Learning and experimentation\n",
    "- Simple tool calling demonstrations\n",
    "- CPU-only environments\n",
    "- Extremely resource-constrained setups\n",
    "- Prototyping and testing\n",
    "- Single-step queries with clear intent\n",
    "\n",
    "‚ùå **Not recommended for:**\n",
    "- Production applications\n",
    "- Complex multi-step reasoning\n",
    "- Nuanced language understanding\n",
    "- Tasks requiring high accuracy\n",
    "- Ambiguous or vague queries\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "**What works well:**\n",
    "- ‚úÖ Simple information retrieval\n",
    "- ‚úÖ Direct tool calls with clear parameters\n",
    "- ‚úÖ Basic question answering\n",
    "- ‚úÖ Straightforward commands\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ö†Ô∏è May select wrong tools occasionally\n",
    "- ‚ö†Ô∏è Struggles with multi-step reasoning\n",
    "- ‚ö†Ô∏è Less natural language generation\n",
    "- ‚ö†Ô∏è May hallucinate or produce inconsistent outputs\n",
    "- ‚ö†Ô∏è Needs very explicit instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Upgrade Path\n",
    "\n",
    "If you find the 270M model too limited, here's the upgrade path:\n",
    "\n",
    "### Step Up: Gemma 2 2B\n",
    "```python\n",
    "# Just change the model name to:\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "```\n",
    "- 7x more parameters\n",
    "- Much better reasoning\n",
    "- Still runs on free Colab\n",
    "- Handles complex queries\n",
    "\n",
    "### Further Up: Gemma 2 9B\n",
    "```python\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "```\n",
    "- 33x more parameters than 270M\n",
    "- Near-API quality\n",
    "- Requires GPU with ~10GB VRAM\n",
    "- Excellent for production use\n",
    "\n",
    "### Other Options\n",
    "- **Qwen2.5-7B-Instruct** - Great balance of size/quality\n",
    "- **Phi-3-mini** - Microsoft's efficient small model\n",
    "- **Llama-3.2-3B** - Meta's balanced option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "‚úÖ **Ultra-Lightweight Model Setup**\n",
    "   - Loading Gemma 3 270M with 4-bit quantization\n",
    "   - Running on CPU or minimal GPU\n",
    "   - Memory management with <1GB usage\n",
    "   - Blazing fast inference\n",
    "\n",
    "‚úÖ **Tool Calling with Small Models**\n",
    "   - Simplifying tool sets for better performance\n",
    "   - Optimizing prompts for small models\n",
    "   - Managing expectations and limitations\n",
    "   - Handling simple, direct queries\n",
    "\n",
    "‚úÖ **Practical Understanding**\n",
    "   - Trade-offs between model size and capability\n",
    "   - When to use ultra-lightweight models\n",
    "   - How to upgrade when needed\n",
    "   - Real-world deployment considerations\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Size Matters**: 270M is incredible for its size, but has real limitations\n",
    "\n",
    "2. **Perfect for Learning**: Ideal for understanding MCP and tool calling mechanics\n",
    "\n",
    "3. **Resource Efficient**: Can run anywhere, even on CPU\n",
    "\n",
    "4. **Know the Limits**: Best for simple, focused tasks\n",
    "\n",
    "5. **Easy to Upgrade**: One line change to use larger, more capable models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Test the limits** - See what works and what doesn't with 270M\n",
    "- **Try Gemma 2 2B** - Experience the dramatic improvement\n",
    "- **Optimize prompts** - Learn how prompt engineering affects small models\n",
    "- **Build hybrid systems** - Use 270M for simple tasks, larger models for complex ones\n",
    "- **Fine-tune** - Improve performance on your specific use case\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You now understand how to work with ultra-lightweight AI models and their trade-offs. This knowledge is valuable for building efficient, resource-conscious AI systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
